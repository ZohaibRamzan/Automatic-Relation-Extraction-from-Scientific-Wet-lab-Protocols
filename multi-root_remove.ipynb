{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove multi-rooted sentences from pre-processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from nltk import word_tokenize     # nltk library used for word tokenization\n",
    "from nltk import tokenize\n",
    "import re                          # re library used for Regular Expression Extraction\n",
    "import nltk\n",
    "import pandas as pd                # pandas used for Data Frames\n",
    "import numpy as np                 # numpy used for array or matrix declaration                  \n",
    "from os import walk                # os used for interacting with the operating system            \n",
    "from nltk.corpus import stopwords  # stopwords of English language\n",
    "import itertools\n",
    "import unittest\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import unicodedata2\n",
    "import time\n",
    "import string\n",
    "from collections import defaultdict,Counter\n",
    "import matplotlib.pyplot as plt    # plotting library for python \n",
    "import seaborn as sns              # data visualization library\n",
    "%matplotlib inline\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for training file\n",
    "with open ('train.json') as jfile:\n",
    "    data=json.load(jfile)\n",
    "    dataF=[]\n",
    "    for ele in data:\n",
    "        if (ele['stanford_deprel'].count('root'))==1:\n",
    "            dataF.append(ele)    \n",
    "jfile.close()\n",
    "dataF=dataF[0:33599]+ dataF[33799:]\n",
    "with open('traine.json', 'w') as fp:\n",
    "        json.dump(dataF, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in dataF:\n",
    "        if (ele['stanford_deprel'].count('root'))>1:\n",
    "            print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for training file\n",
    "with open ('dev.json') as jfile:\n",
    "    data=json.load(jfile)\n",
    "    dataB=[]\n",
    "    for ele in data:\n",
    "        if (ele['stanford_deprel'].count('root'))==1:\n",
    "            \n",
    "            dataB.append(ele)    \n",
    "jfile.close()\n",
    "with open('devi.json', 'w') as fp:\n",
    "        json.dump(dataB, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devi.json') as d:\n",
    "    dat=json.load(d)\n",
    "    for ele in dat:\n",
    "        if (ele['stanford_deprel'].count('root'))>1:\n",
    "            print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for test file\n",
    "with open ('test.json') as jfile:\n",
    "    data=json.load(jfile)\n",
    "    dataA=[]\n",
    "    for ele in data:\n",
    "        if (ele['stanford_deprel'].count('root'))==1:\n",
    "            \n",
    "            dataA.append(ele)    \n",
    "jfile.close()\n",
    "with open('testi.json', 'w') as fp:\n",
    "        json.dump(dataA, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_pytorch",
   "language": "python",
   "name": "gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
